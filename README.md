# Formacao_ADA_Modulos
Este reposit√≥rio cont√©m os projetos desenvolvidos no decorrer da Forma√ß√£o ADA de Data Science

# Projeto_L√≥gica_de_Programa√ß√£o_II
Nesse projeto iremos focar no conhecimento adquirido durante o m√≥dulo de l√≥gica de programa√ß√£o II.

Utilizaremos as estruturas de dados (tuplas, listas, dicion√°rios) al√©m da l√≥gica de programa√ß√£o (if/else, while, e for), lembre-se das compreens√µes de listas e dicion√°rios. E utilize as t√©cnicas e algoritmos que achar necess√°rio para realizar o projeto. A ideia √© que no projeto sejam utilizados o maior n√∫mero poss√≠vel de recursos que foram aprendidos durante o m√≥dulo.

### Crit√©rios de avalia√ß√£o

Os seguintes itens ser√£o avaliados:

1. Reprodutibilidade do c√≥digo: seu c√≥digo ser√° executado e precisa gerar os mesmos resultados apresentados por voc√™;

2. Clareza: seu c√≥digo precisa ser claro e deve existir uma linha de racioc√≠nio direta. Comente o c√≥digo em pontos que julgar necess√°rio para o entendimento total;

3. Utiliza√ß√£o dos recursos aprendidos durante o m√≥dulo.


### Informa√ß√µes gerais

- O projeto pode ser desenvolvido individualmente ou em grupo

- Entrega (em defini√ß√£o).

Anexar, na entrega, o c√≥digo (notebook ou script python) desenvolvido e os arquivos gerados.

### Tarefas: Iniciando Constru√ß√£o do Projeto

#### 1. Criar uma estrutura de dados que armazene as vendas realizadas em uma loja.

#### 2. Implemente uma fun√ß√£o que solicite ao usu√°rio a a√ß√£o desejada (Incluir ou Excluir) um item da lista. 

    Informa√ß√µes sobre a venda que devem constar:

    - ID (incremental)
    - Nomes dos produtos
    - Quantidade de Produtos
    - Valores dos produtos
    - Nome do vendedores
    - CPF dos compradores
    - Datas das vendas
    - Informa√ß√µes opcionais

# Projeto_T√©cnicas_Programa√ß√£o_II

**Execu√ß√£o**

-Para rodar o arquivo localmente, certifique-se que as vari√°veis "descompactar" (notebook dos pre√ßos) ou a vari√°vel "rodando_localmente" (notebook dos tributos) est√£o marcadas como True

-Certifique-se tamb√©m de instalar as bibliotecas: pandas, numpy, matplotlib, seaborn, geopandas, plotly, xlrd e mpl_toolkits no seu ENV 

-Realizamos esse trabalho em conjunto partindo de um projeto da iniciativa Santander Coders 2023, lecionados pela ADA Tech na disciplina T√©cnicas de Programa√ß√£o I

**Projeto - T√©cnicas de Programa√ß√£o I**

-Instrutor: Alex Lima

-Auxiliar: Maur√≠cio Sobrinho

**Equipe: Gabriel Miranda, Marcus Thadeu, Ruann Campos e Thiago Caveglion**

O projeto tem como objetivos:
* Analisar de forma explorat√≥ria a s√©rie hist√≥rica de pre√ßos de combust√≠veis no Brasil no per√≠odo de 2018.1 a 2023.1;
* Analisar de forma explorat√≥ria a s√©rie hist√≥rica de arrecada√ß√µes tribut√°rias estaduais no mesmo per√≠odo;
* Relacionar ambos datasets.

**Informa√ß√µes importantes sobre os dois notebooks do projeto:**

- An√°lise_Explorat√≥ria_Combust√≠veis.ipynb: notebook onde constru√≠mos a an√°lise explorat√≥ria sobre os combust√≠veis.
  - Pasta datasets_combust√≠vel_comprimido: onde est√° armazenado os datasets comprimidos dos combust√≠veis. Dados dispon√≠veis em: https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis.
  - Pasta IBGE_Aux: onde est√° armazenado o dataset auxiliar com os dados de geolocaliza√ß√£o utilizado para a constru√ß√£o dos mapas na an√°lise.
-  An√°lise_Explorat√≥ria_Tributa√ß√£o.ipynb: notebook onde constru√≠mos a an√°lise explorat√≥ria sobre as tributa√ß√µes dos estados.
    - Dataset_Tributos.xls: dataset utilizado para aquis√£o de dados referentes √†s tributa√ß√µes (dados dispon√≠veis em: https://dados.gov.br/dados/conjuntos-dados/boletim-de-arrecadacao-dos-tributos-estaduais)

# Projeto Banco de Dados I
### Modelagem e normaliza√ß√£o de bancos de dados relacionais
Certo dia, um dos gestores do banco em que voc√™ trabalha como cientista de dados procurou voc√™ pedindo ajuda para projetar um pequeno banco de dados com o objetivo de mapear os clientes da companhia pelos diferentes produtos financeiros que eles contrataram.

O gestor explicou que o banco tinha uma grande quantidade de clientes e oferecia uma variedade de produtos financeiros, como cart√µes de cr√©dito, empr√©stimos, seguros e investimentos. No entanto, eles estavam tendo dificuldades para entender quais produtos eram mais populares entre os clientes e como esses produtos estavam interagindo entre si.

Como ponto de partida, o gestor deixou claro que um cliente pode contratar v√°rios produtos diferentes ao passo que um mesmo produto pode tamb√©m estar associado a v√°rios clientes diferentes e elaborou um r√∫stico esbo√ßo de banco de dados com duas tabelas, da seguinte forma:

Tabela 1

    - Nome da tabela: cliente
    - Colunas: codigo_cliente, nome_cliente, sobrenome_cliente, telefones_cliente, municipio_cliente, codigo_tipo_cliente, tipo_cliente
Tabela 2

    - Nome da tabela: produto
    - Colunas: codigo_produto, nome_produto, descricao_produto, codigo_tipo_produto, tipo_produto, codigo_diretor_responsavel, nome_diretor_responsavel, email_diretor_responsavel

##### 1) Ainda sem fazer normaliza√ß√µes, apresente o modelo conceitual deste esbo√ßo oferecido pelo gestor, destacando atributos chaves e multivalorados, caso existam, e apresentando tamb√©m a cardinalidade dos relacionamentos.

##### 2) Agora apresente um modelo l√≥gico que expresse as mesmas informa√ß√µes e relacionamentos descritos no modelo original, mas decompondo-os quando necess√°rio para que sejam respeitadas as 3 primeiras formas normais. Destaque atributos chaves e multivalorados, caso existam, e apresente tamb√©m a cardinalidade dos relacionamentos.

#### Consultas SQL simples e complexas em um banco de dados postgres
Um exemplo de modelo de banco de dados com relacionamento muitos-para-muitos pode ser o de um e-commerce que tem produtos e categorias, onde um produto pode pertencer a v√°rias categorias e uma categoria pode estar associada a v√°rios produtos. Nesse caso, ter√≠amos duas tabelas: "produtos" e "categorias", com uma tabela intermedi√°ria "produtos_categorias" para relacionar os produtos √†s suas categorias.

    CREATE TABLE produtos (
    id SERIAL PRIMARY KEY,
    nome VARCHAR(100) NOT NULL,
    preco DECIMAL(10, 2) NOT NULL,
    );

    CREATE TABLE categorias (
    id SERIAL PRIMARY KEY,
    nome VARCHAR(100) NOT NULL
    );

    CREATE TABLE produtos_categorias (
    produto_id INTEGER REFERENCES produtos(id),
    categoria_id INTEGER REFERENCES categorias(id),
    PRIMARY KEY (produto_id, categoria_id)
    );
Assim, usando o subconjunto da "structured query language" chamado de DQL, produza consultas postgress de modo a atender cada uma das seguintes solicita√ß√µes:

##### 3) Liste os nomes de todos os produtos que custam mais de 100 reais, ordenando-os primeiramente pelo pre√ßo e em segundo lugar pelo nome. Use alias para mostrar o nome da coluna nome como "Produto" e da coluna preco como "Valor". A resposta da consulta n√£o deve mostrar outras colunas de dados.

##### 4) Liste todos os ids e pre√ßos de produtos cujo pre√ßo seja maior do que a m√©dia de todos os pre√ßos encontrados na tabela "produtos".

##### 5) Para cada categoria, mostre o pre√ßo m√©dio do conjunto de produtos a ela associados. Caso uma categoria n√£o tenha nenhum produto a ela associada, esta categoria n√£o deve aparecer no resultado final. A consulta deve estar ordenada pelos nomes das categorias.

#### Inser√ß√µes, altera√ß√µes e remo√ß√µes de objetos e dados em um banco de dados postgres
Voc√™ est√° participando de um processo seletivo para trabalhar como cientista de dados na Ada, uma das maiores formadoras do pa√≠s em √°reas correlatadas √† tecnologia. Dividido em algumas etapas, o processo tem o objetivo de avaliar voc√™ nos quesitos Python, Machine Learning e Bancos de Dados. Ainda que os dois primeiros sejam o cerne da sua atua√ß√£o no dia-a-dia, considera-se que Bancos de Dados tamb√©m constituem um requisito importante e, por isso, esta etapa pode ser a oportunidade que voc√™ precisava para se destacar dentre os seus concorrentes, demonstrando um conhecimento mais amplo do que os demais.

##### 6) Com o objetivo de demonstrar o seu conhecimento atrav√©s de um exemplo contextualizado com o dia-a-dia da escola, utilize os comandos do subgrupo de fun√ß√µes DDL para construir o banco de dados simples abaixo, que representa um relacionamento do tipo 1,n entre as entidades "aluno" e "turma":

Tabela 1

    - Nome da tabela: aluno
    - Colunas da tabela: id_aluno (INT), nome_aluno (VARCHAR), aluno_alocado (BOOLEAN), id_turma (INT)
Tabela 2

    - Nome da tabela: turma
    - Colunas da tabela: id_turma (INT), c√≥digo_turma (VARCHAR), nome_turma (VARCHAR)
##### 7) Agora que voc√™ demonstrou que consegue ser mais do que um simples usu√°rio do banco de dados, mostre separadamente cada um dos c√≥digos DML necess√°rios para cumprir cada uma das etapas a seguir:
    a) Inserir pelo menos duas turmas diferentes na tabela de turma;
    b) Inserir pelo menos 1 aluno alocado em cada uma destas turmas na tabela aluno (todos com NULL na coluna aluno_alocado);
    c) Inserir pelo menos 2 alunos n√£o alocados em nenhuma turma na tabela aluno (todos com NULL na coluna aluno_alocado);
    d) Atualizar a coluna aluno_alocado da tabela aluno, de modo que os alunos associados a uma disciplina recebam o valor True e alunos n√£o associdos a nenhuma disciplina recebam o falor False para esta coluna.

# Projeto_Estat√≠stica
# An√°lise Explorat√≥ria Estat√≠stica dos Correios
An√°lise do tr√°fego dos correios no ano de 2023, ponderando sobre m√©tricas escolhidas e levantando pontos relevantes.
## Equipe
A equipe que realizou este projeto √© composta por: Gabriel Miranda, Marcus Thadeu, Ruann Campos e Thiago Caveglion
## Orientador
O orientador deste projeto foi Felipe Yoshimoto, professor na Ada Tech no programa Santander Coders 2023 na disciplina de Estat√≠stica I
## Estrutura
Os arquivos est√£o organizados em um jupyter notebook e um arquivo .parquet com os dados
## M√©todo
Realizamos an√°lises estat√≠sticas para fazer uma an√°lise explorat√≥ria baseada num dataset mascarado de tr√°fego dos correios no ano de 2023
## Bibliotecas
As bibliotecas utilizadas foram indicadas no arquivo ipynb
##

# [Detec√ß√£o de fraude em transa√ß√µes de cart√µes de cr√©dito utilizando modelos classificat√≥rios de Machine Learning](https://github.com/grmirand4/sc2023-deteccao-fraude-machine-learning)

**Projeto - Machine Learning I**

Instrutor: [Carlos Stefano](https://www.linkedin.com/in/carlos-stefano/)

**Equipe: [Gabriel Miranda](https://www.linkedin.com/in/grmiranda/), [Marcus Thadeu](https://www.linkedin.com/in/marcus-thadeu/), [Ruann Campos](https://www.linkedin.com/in/ruann-campos/) e [Thiago Caveglion](https://www.linkedin.com/in/thiago-caveglion/)**

## üéØ Objetivo geral
Prever transa√ß√µes fraudulentas em cart√µes de cr√©dito atrav√©s da constru√ß√£o de modelos classificat√≥rios de machine learning (KNN, Decision Tree, Random Forest e Gradient Boosting)

## üìù Objetivos espec√≠ficos
No desenvolvimento de nossa an√°lise, buscamos responder √†s seguintes perguntas:
* √â poss√≠vel prever quais das transa√ß√µes realizadas entre 06/2020 e 12/2020 contidas em `fraudTest.csv` s√£o fraudulentas?
* Quais dos modelos classificat√≥rios constru√≠dos apresentaram melhor performance?

## üìä Sobre o data set
Os data sets `fraudTrain.csv` e `fraudTest.csv` utilizados em nossa an√°lise apresentam informa√ß√µes fict√≠cias retiradas [dessa base de dados do Kaggle](https://www.kaggle.com/datasets/kartik2112/fraud-detection). √â um data set simulado de transa√ß√µes de cart√µes de cr√©dito ocorridas entre as datas de 01/01/2019 a 31/12/2020; refere-se aos cart√µes de 1000 clientes diferentes realizando transa√ß√µes a 800 comerciantes distintos. As colunas dos arquivos incluem:
* `trans_date_trans_time`: data e hora da transa√ß√£o
* `cc_num`: n√∫mero do cart√£o de cr√©dito
* `merchant`: nome do comerciante
* `category`: categoria de compra
* `amt`: valor da transa√ß√£o
* `first`: primeiro nome da pessoa
* `last`: √∫ltimo nome da pessoa
* `gender`: g√™nero da pessoa que realizou a transa√ß√£o
* `dob`: data de nascimento da pessoa que realizou a transa√ß√£o
* `job`: profiss√£o da pessoa que realizou a transa√ß√£o
* `street`, `state`, `zip`: endere√ßo da pessoa que realizou a transa√ß√£o
* `city`: cidade da pessoa que realizou a transa√ß√£o
* `city_pop`: popula√ß√£o da cidade da pessoa que realizou a transa√ß√£o
* `lat`: latitude da pessoa que realizou a transa√ß√£o
* `long`: longitude da pessoa que realizou a transa√ß√£o
* `merch_lat`: latitude do comerciante
* `merch_long`: longitude do comerciante
* `trans_num`, `unix_time`: dados da transa√ß√£o
* `is_fraud`: 1 para transa√ß√µes fraudulentas, 0 caso contr√°rio

## üí° Principais conclus√µes
* De forma geral, fomos capazes de utilizar os diferentes modelos de classifica√ß√£o propostos (KNN, Decision Tree, Random Forest e Gradient Boosting) para a predi√ß√£o de fraudes dados os data sets trabalhados.
  
**Sobre o desempenho de cada modelo:**

* √â poss√≠vel que o **KNN** n√£o seja o melhor modelo para este problema (ou que os modelos trabalhados precisem de mais ajustes).
  * O **recall baixo** (`recall = 0.3` e `recall = 0.25`) representa uma preocupa√ß√£o. Em contextos como detec√ß√£o de fraudes, devemos priorizar um recall mais alto, mesmo que isso signifique sacrificar um pouco a precis√£o. Falsos negativos (fraudes n√£o detectadas) podem ter consequ√™ncias mais graves do que falsos positivos.
  * A **baixa precis√£o** dos modelos sugere que ainda h√° espa√ßo para melhorar a capacidade de se evitar falsos positivos.
    
* **Decision Tree:** foi poss√≠vel criar um modelo com cerca de 92% de precis√£o e 72% de recall, totalizando um `f1-score` de cerca de 0.81 com uma curva ROC de 0.96 de √°rea.

* **Random Forest:** com a otimiza√ß√£o dos hiperpar√¢metros, o modelo atingiu uma acur√°cia de 0.99.
	* Sobre a classifica√ß√£o: baixa precis√£o para a classe 1, onde apenas 30% dos dados classificados como fraude realmente a s√£o.
	* Recall com bom desempenho, onde o modelo √© capaz de identificar 85% dos verdadeiros positivos.
	* F1-Score: m√©dia entre precision e recall de 0.45.

* O modelo **Gradient Boosting Otimizado** apresenta uma maior quantidade de acertos na classe de verdadeiros positivos (`recall = 0.84`).
	* No entanto, pode ser √∫til investigar maneiras de melhorar a precis√£o (`precision = 0.45`) sem sacrificar muito o recall, para reduzir o n√∫mero de transa√ß√µes leg√≠timas classificadas erroneamente como fraudulentas.

 * Conclu√≠mos que, dentro dos modelos trabalhados aqui, o **Decision Tree apresentou o melhor desempenho para o nosso conjunto de dados**.

**Pontos a serem melhorados:**

* Para classifica√ß√£o dos dados: utilizar t√©cnicas para lidar com a falta de balanceamento dos data sets (resampling, por exemplo).
* Utilizar as mesmas features para os diferentes modelos de forma a gerar compara√ß√µes mais fidedignas.
* Adicionar outras m√©tricas de compara√ß√£o, como a pr√≥pria curva ROC.

## üíª Principais linguagens
- Python
  - Pandas
  - Numpy
  - Seaborn
  - Plotly
  - Matplotlib
  - Scipy
  - Sklearn
  - Imblearn
  - Lightgbm
  - Joblib

## üë®‚Äçüíª Execu√ß√£o
Para executar os notebooks localmente, certifique-se de:

* Fazer o download dos arquivos `fraudTrain.csv` e `fraudTest.csv` [neste link do Kaggle](https://www.kaggle.com/datasets/kartik2112/fraud-detection) (eles s√£o muito grandes para serem colocados neste reposit√≥rio).
* Alterar o caminho nas linhas de c√≥digo que l√™em os data sets: `df_train = pd.read_csv("caminho/fraudTrain.csv")` e `df_test = pd.read_csv("caminho/fraudTest.csv")`.

Neste reposit√≥rio, voc√™ encontra 4 notebooks diferentes, cada um focado em um dos modelos (KNN, Decision Tree, Random Forest e Gradient Boosting).

###### Tags: `python` `data-science` `random-forest` `credit-card` `knn` `decision-tree` `fraud-detection` `gradient-boosting`
